{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72632,"databundleVersionId":8059709,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\n\nfrom imblearn.over_sampling import SMOTE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the datasets","metadata":{}},{"cell_type":"markdown","source":"### Change the path of the files accordingly","metadata":{}},{"cell_type":"code","source":"train_data_file = '/kaggle/input/who-is-the-real-winner/train.csv'\ntest_data_file = '/kaggle/input/who-is-the-real-winner/test.csv'\n\ntrain_df = pd.read_csv(train_data_file)\ntest_df = pd.read_csv(test_data_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is how our train data looks like!","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is how our test data looks like!","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states = np.array(train_df['state'].unique())\nparties = np.array(train_df['Party'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Define functions for data preprocessing","metadata":{}},{"cell_type":"code","source":"def convert_to_numeric(value):\n    if 'Crore' in value:\n        return int(float(value.replace(' Crore+', '')) * 10000000)\n    elif 'Lac' in value:\n        return int(float(value.replace(' Lac+', '')) * 100000)\n    elif 'Thou' in value:\n        return int(float(value.replace(' Thou+', '')) * 1000)\n    elif 'Hund' in value:\n        return int(float(value.replace(' Hund+', '')) * 100)\n    elif value == '0':\n        return int(0)\n\ndef preprocess_data(df):\n    # Convert string data to numeric data\n    df['Total Assets'] = df['Total Assets'].apply(convert_to_numeric)\n    df['Liabilities'] = df['Liabilities'].apply(convert_to_numeric)\n    df['Criminal Case'] = df['Criminal Case'].apply(lambda x: 1 if x > 0 else 0)\n    # One hot encode categorical data\n    coded1 = pd.get_dummies(df['Party']).astype(int)\n    coded2 = pd.get_dummies(df['state']).astype(int)\n    df = pd.concat([df, coded1, coded2], axis=1)\n    \n    # Drop the unwanted columns\n    df = df.drop(['ID', 'Party', 'state'], axis = 1)\n    \n    return df\n\ndef encode_education(df):\n    # label encode education\n    # higher degree of education is marked by a higher number\n    label_mapping = {'Others': 0, 'Literate': 1, '5th Pass': 2, '8th Pass': 3, '10th Pass': 4,\n                 '12th Pass': 5, 'Graduate': 6, 'Graduate Professional': 7,\n                 'Post Graduate': 8, 'Doctorate': 9}\n    df['Education'] = df['Education'].map(label_mapping)\n    return df\n\ndef decode_education(arr):\n    # Decode the education back for submission\n    label_mapping = {0: 'Others', 1: 'Literate', 2: '5th Pass', 3: '8th Pass', 4: '10th Pass',\n                     5: '12th Pass', 6: 'Graduate', 7: 'Graduate Professional',\n                     8: 'Post Graduate', 9: 'Doctorate'}\n    return np.vectorize(label_mapping.get)(arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess the train and test data.","metadata":{}},{"cell_type":"code","source":"train_df = preprocess_data(train_df)\ntrain_df = encode_education(train_df)\ntest_df = preprocess_data(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is how our train data looks like after preprocessing!","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is how our test data looks like after preprocessing!","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Define functions for making new features","metadata":{}},{"cell_type":"code","source":"# make new features\ndef make_features(df):\n    df['ALRatio'] = df['Total Assets']/(df['Liabilities']+1) # Add 1 to handle 0 values\n    df['Net Assets'] = df['Total Assets'] - df['Liabilities']\n    \n    df['Advocate'] = df['Candidate'].str.contains('Adv.', case=False).astype(int)\n    df['Dr'] = df['Candidate'].str.contains('Dr.', case=False).astype(int)\n    df = df.drop(['Candidate'], axis=1)\n    return df\n\ndef scale_features(df):\n    scaler = MinMaxScaler()\n#     scaler = StandardScaler()\n    df[\"Scaled Total Assets\"] = scaler.fit_transform(df[['Total Assets']])\n    df[\"Scaled Liabilities\"] = scaler.fit_transform(df[['Liabilities']])\n    df[\"Scaled Net Assets\"] = scaler.fit_transform(df[['Net Assets']])\n    return df\n\ndef log_features(df):\n    # Scale logarithmically\n    take_log = lambda value: 0 if value == 0 else np.log(value)\n    df['Log Total Assets'] = df['Total Assets'].apply(take_log)\n    df['Log Liabilities'] = df['Liabilities'].apply(take_log)\n    df['Log AL Ratio'] = df['Log Total Assets'] - df['Log Liabilities']\n    return df\n\ndef group_states(df):\n    region_mapping = {\n        'South': ['TAMIL NADU', 'KARNATAKA', 'KERALA', 'ANDHRA PRADESH', 'PUDUCHERRY'],\n        'North': ['UTTAR PRADESH', 'PUNJAB', 'HARYANA', 'DELHI', 'HIMACHAL PRADESH', 'RAJASTHAN', 'UTTARAKHAND'],\n        'West': ['MAHARASHTRA', 'GUJARAT', 'GOA'],\n        'East': ['WEST BENGAL', 'ODISHA', 'JHARKHAND', 'BIHAR'],\n        'North East': ['MEGHALAYA', 'MANIPUR', 'NAGALAND', 'SIKKIM', 'TRIPURA', 'ASSAM', 'ARUNACHAL PRADESH'],\n        'Central': ['MADHYA PRADESH', 'CHHATTISGARH']\n    }\n\n    for region, states in region_mapping.items():\n        df[region] = df[states].sum(axis=1)  # Sum the columns for each region and add as a new column to df\n\n    return df\n\ndef group_constituencies(df):\n    categorize = lambda constituency: 'SC' if 'SC' in constituency else ('ST' if 'ST' in constituency else 'General')\n    df[\"Category\"] = df['Constituency ∇'].apply(categorize)\n    \n    # One hot encode Category\n    coded = pd.get_dummies(df['Category']).astype(int)\n    df = pd.concat([df, coded], axis=1)\n    \n    # Drop the unwanted columns\n    df = df.drop(['Category', 'Constituency ∇'], axis = 1)\n    \n    return df\n\ndef group_regional_parties(df):\n    region_mapping = {\n        'North Parties': ['AAP', 'JD(U)', 'SHS', 'TDP'],\n        'South Parties': ['AIADMK', 'DMK', 'JD(S)', 'YSRCP', 'Sikkim Krantikari Morcha', 'Tipra Motha Party', 'SP'],\n        'East Parties': ['AITC', 'BJD', 'CPI', 'CPI(M)', 'RJD'],\n        'West Parties': ['NCP', 'NDPP'],\n        'North East Parties': ['IND', 'JMM', 'NPP'],\n        'Nationwide Parties': ['BJP', 'INC'],\n    }\n    \n    for region, parties in region_mapping.items():\n        df[region] = df[parties].sum(axis=1)  # Sum the columns for each region and add as a new column to df\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make new features from existing features","metadata":{}},{"cell_type":"code","source":"train_df = make_features(train_df)\ntrain_df = scale_features(train_df)\ntrain_df = log_features(train_df)\ntrain_df = group_states(train_df)\ntrain_df = group_constituencies(train_df)\ntrain_df = group_regional_parties(train_df)\n\ntest_df = make_features(test_df)\ntest_df = scale_features(test_df)\ntest_df = log_features(test_df)\ntest_df = group_states(test_df)\ntest_df = group_constituencies(test_df)\ntest_df = group_regional_parties(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is how our train data looks after adding new features","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is how our test data looks after adding new features","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let us see the correlation of various features with Education\n\n#### We can divide features into two categories - Numerical and Cagtegorical","metadata":{}},{"cell_type":"code","source":"all_features = train_df.columns.tolist()\nall_features.remove('Education')\n\nnumerical_features = ['Total Assets', 'Liabilities', 'Net Assets', 'ALRatio',\n                      'Scaled Total Assets', 'Scaled Liabilities', 'Scaled Net Assets', \n                      'Log Total Assets', 'Log Liabilities', 'Log AL Ratio']\n\ncategorical_features = [feature for feature in all_features if feature not in numerical_features]\n\nprint(\"Numerical Features:\", numerical_features)\nprint()\nprint(\"Categorical Features:\", categorical_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation of Education with Numerical Features","metadata":{}},{"cell_type":"code","source":"num_corr = train_df[numerical_features].corrwith(train_df['Education'])\nprint(num_corr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation of Education with categorical Features","metadata":{}},{"cell_type":"code","source":"categ_corr = train_df[categorical_features].corrwith(train_df['Education'])\nn = categ_corr.shape[0]\nm = n//2\nprint(categ_corr.iloc[0:m])\nprint(categ_corr.iloc[m:n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observethat Scaling does not affect the correlation, but it may help in decreasing training time. So let us keep only the scaled features and remove the original ones","metadata":{}},{"cell_type":"code","source":"unwanted_features = np.array(['Criminal Case', 'Total Assets', 'Liabilities'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we have multiple options - \n#### Option 1 : Remove individual states and only keep grouped states\n#### Option 2 : Remove individual parties and only keep grouped parties\n#### Option 3 : Remove state and parties with a very high or very less number of entries in the dataset.\n#### Option 4 : Use sklearn.feature_selection to select k best features\n\n#### We can also use a combination of these options","metadata":{}},{"cell_type":"markdown","source":"##### Option 1","metadata":{}},{"cell_type":"code","source":"unwanted_features = np.concatenate((unwanted_features, states))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Option 2","metadata":{}},{"cell_type":"code","source":"unwanted_features = np.concatenate((unwanted_features, parties))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Option 3","metadata":{}},{"cell_type":"code","source":"train_copy = pd.read_csv(train_data_file)\nprint(train_copy['Party'].value_counts())\nprint(train_copy['state'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_parties = np.array(['Tipra Motha Party', 'CPI', 'Sikkim Krantikari Morcha', \n               'JD(S)', 'TDP', 'NDPP', 'BJP', 'INC', 'JMM'])\n\n\nunwanted_features = np.concatenate((unwanted_features, drop_parties))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_states = np.array(['UTTAR PRADESH','WEST BENGAL','MAHARASHTRA','MADHYA PRADESH',\n                        'KARNATAKA','TAMIL NADU','BIHAR','GUJARAT','RAJASTHAN','SIKKIM',\n                        'GOA','MEGHALAYA','PUDUCHERRY'])\n\nunwanted_features = np.concatenate((unwanted_features, drop_states))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Option 4","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(unwanted_features, axis = 1)\ntest_df = test_df.drop(unwanted_features, axis = 1)\nunwanted_features = np.array([])\n# Separate the features and target variable\nX = train_df.drop('Education', axis=1)\ny = train_df['Education']\n\n# Select k best features using SelectKBest and f_classif\nk = 40  # Number of features you want to select\nselector = SelectKBest(score_func=f_classif, k=k if X.shape[1] >= k else X.shape[1])\nX_new = selector.fit_transform(X, y)\n\n# Get the list of selected features\nselected_features_mask = selector.get_support()\nselected_features = np.array(X.columns[selected_features_mask])\n\n# Get the list of features that were not selected\nnot_selected_features = np.array(X.columns[~selected_features_mask])\n\nunwanted_features = np.concatenate((unwanted_features, not_selected_features))\n\nprint(\"Selected Features:\", selected_features.tolist())\nprint()\nprint(\"Not Selected Features:\", not_selected_features.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Let us look at all the unwanted features","metadata":{}},{"cell_type":"code","source":"print(unwanted_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop the unwanted features","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(unwanted_features, axis = 1)\ntest_df = test_df.drop(unwanted_features, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is how our train data looks like after removing the unwanted features","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is how our test data looks like after removing the unwanted features","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling using SMOTE","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(['Education'], axis=1)\ny = train_df['Education']\n\n# Define the sampling strategy for SMOTE\nsampling_strategy = {0:100, 1:60, 2:40, 3:1000, 4:1000, 5:1000, 6:1000, 7:1000, 8:1000, 9:1000}\n\n# Initialize SMOTE with the specified sampling strategy\nsmote = SMOTE(sampling_strategy=sampling_strategy)\n\n# Apply SMOTE to generate synthetic samples\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Create a new DataFrame with the resampled data\nresampled_df = pd.DataFrame(X_resampled, columns=X.columns)\nresampled_df['Education'] = y_resampled\n\n# Print the value counts of the target variable after resampling\nprint(resampled_df['Education'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy resampled data to train data\ntrain_df = resampled_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate resampled data to train data\ntrain_df = pd.concat([train_df, resampled_df], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This is how our training data looks like after oversampling","metadata":{}},{"cell_type":"code","source":"print(train_df.head())\nprint(train_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning : Finding best params","metadata":{}},{"cell_type":"code","source":"# Get a copy of the training data\ndf = train_df.copy()\n\n# Calculating prior for BernoulliNB\n# Calculate class frequencies from y_train\nclass_frequencies = df['Education'].value_counts(normalize=True)\n# Calculate class probabilities\nclass_probabilities = class_frequencies.sort_index().values\n\nclassifiers_list = [\n    {'classifier': BernoulliNB(),\n     'param_grid': {\n         'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, \n                   1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n         'force_alpha': [True, False],\n         'fit_prior': [True, False],\n         'class_prior': [None, class_probabilities],\n     }},\n    {'classifier': RandomForestClassifier(random_state=42),\n     'param_grid': {\n         'n_estimators': [55, 60, 65],\n         'max_depth': [14, 15, 16],\n         'criterion': ['gini', 'entropy'],\n         'min_samples_split': [9, 10, 11],\n         'min_samples_leaf': [0.001, 0.01, 0.1],\n         'random_state': [42]\n     }},\n    {'classifier': DecisionTreeClassifier(random_state=42),\n     'param_grid': {\n         'max_depth': [14, 15, 16],\n         'criterion': ['gini', 'entropy'],\n         'splitter': ['best', 'random'],\n         'min_samples_split': [9, 10, 11],\n         'min_samples_leaf': [0.001, 0.01, 0.1],\n         'random_state': [42]\n     }},\n    {'classifier': KNeighborsClassifier(),\n     'param_grid': {\n         'n_neighbors': [3, 5, 7],\n         'weights': ['uniform', 'distance'],\n         'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n     }},\n    {'classifier': SVC(),\n     'param_grid': {\n         'C': [10,100,1000],\n         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n         'degree': [1,2,3],\n         'gamma': ['scale', 'auto'],\n         'random_state': [42]\n     }},\n    \n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df.drop(['Education'], axis=1)\ny_train = df['Education']\n\nresults = []\nbest_models = []\n\nfor item in classifiers_list:\n    clf = item['classifier']\n    param_grid = item['param_grid']\n    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_weighted')\n    grid_search.fit(X_train, y_train)\n    results.append({'classifier': clf.__class__.__name__, 'best_params': grid_search.best_params_, 'best_score': grid_search.best_score_})\n    best_models.append(grid_search.best_estimator_)\n\nfor idx, result in enumerate(results, start=1):\n    print(idx)\n    print(f\"Classifier: {result['classifier']}\")\n    print(f\"Best Params: {result['best_params']}\")\n    print(f\"F1 Score: {result['best_score']}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, validating the models with best params and creating submissions files","metadata":{}},{"cell_type":"code","source":"def train_validate_submit(df, model, filename):\n    X = df.drop(['Education'], axis=1)\n    y = df['Education']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = model\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    \n    model_name = clf.__class__.__name__\n    model_params = clf.get_params()\n    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f\"Classifier: {model_name}\")\n    print(f\"Best Params: {model_params}\")\n    print(f\"F1 Score: {weighted_f1}\")\n    print()\n    \n    y_pred = clf.predict(test_df)\n    y_pred = decode_education(y_pred)\n    filename = filename\n    df_submit = pd.DataFrame({'ID' : range(len(y_pred)), 'Education' : y_pred})\n    df_submit.to_csv(f'{filename}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = '' # enter the trial number for distinguishing multiple submissions of same model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy()\nfor model in best_models:\n    filename = model.__class__.__name__ + mode\n    train_validate_submit(df,model, filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}